# CircRM

## Brief introduction

CircRM is a computational tool for profiling circular RNAs (circRNAs) using Nanopore direct RNA sequencing (DRS) data. It can detect circRNAs and identify RNA modifications (m⁶A, m¹A, and m⁵C) at single-molecule, single-base resolution. Designed for flexibility, circRM works across multiple cell lines and species, enabling comprehensive analysis of circRNA epitranscriptomes.

## Installation

```bash
git clone https://github.com/jiayiAnnie17/CircRM
```

> **Note:** Please make sure to have some basic Python packages installed, such as `os`, `pandas`, `argparse`, and `sys`.

## 1 Profiling of circRNAs

### 1.1 Prepare virtual library

```bash
Rscript ./CircRM/Code/Build_VL.R <TxDb_annotation> <output_csv>
```

#### Dependencies

Make sure the following R packages are installed before running the script: `GenomicRanges`, `GenomicFeatures`, `rtracklayer`, `data.table`, `tidyr`.

#### Parameter explaination

- `<TxDb_annotation>`: a transcript annotation object from a TxDb package.
  Examples:
   - `TxDb.Mmusculus.UCSC.mm10.knownGene` for mouse
   - `TxDb.Hsapiens.UCSC.hg38.knownGene` for human
- `<output_csv>`: the CSV file generated by this script

The script accepts any species and genome assembly for which a TxDb annotation package is available; just provide the appropriate TxDb object. Make sure the corresponding TxDb package is installed before running the script.

#### Output file format

The output file contains six columns as follows:

| Column | Description |
|--------|-------------|
| `Transcript_ID:Location` | Transcript ID and genomic coordinates (e.g., `ENST00000456328.2chr1:11869-12227`) |
| `Chromosome` | Chromosome name (e.g., `chr1`) |
| `Start` | Start position of the transcript or feature (e.g., `11869`) |
| `End` | End position of the transcript or feature (e.g., `12227`) |
| `Strand` | Strand information (`+` or `-`) |
| `Transcript_ID` | Transcript ID (e.g., `ENST00000456328.2`) |


### 1.2 Split

This step groups the input CSV by transcript, enabling efficient processing of large files.

```bash
python ./CircRM/Code/Split.py \
  -i <input_csv> \
  -o <output_directory> \
  [-b <batch_size>]
```

#### Parameter explaination

- `-i`: the CSV file generated by the previous step
- `-o`: the folder where the output files will be saved
- `-b` : optional, the number of transcripts per batch (default: 50)

### 1.3 Find circRNAs

**Note:** If FASTQ files are not yet available, you can generate them using **2.2 Base calling** from the pass files, and then return to this step.

```bash
python ./CircRM/Code/find_circRNA.py \
  -i <input_fastq> \
  -d <csv_directory> \
  -r <reference_fasta> \
  -o <output_file> \
  [-t <threads>] \
  [-m <merged_fasta>]
```

#### Dependencies

This script requires the following Python packages: `tqdm`, `biopython` (for `Bio`), and `mappy`.

#### Parameters explaination

- `-i` : Path to the input FASTQ file containing sequencing reads
- `-d` : Path to the directory containing CSV files required for processing
- `-r` : Reference genome FASTA file (e.g., hg38.fa)
- `-o` : Output file to save circRNA detection results
- `-t` : Number of threads to use (default: 1)
- `-m` : Output FASTA file name for merged sequences (default: `merged_sequences.fasta`)

#### Output file format

The output file contains three columns:

| Column | Description |
|--------|-------------|
| `Read_ID` | Unique identifier of the read (e.g., `8419b769-3b61-422a-9188-543e793680a2`) |
| `Transcript_Location` | Transcript ID and genomic coordinates (e.g., `ENST00000459737.5chr14:55650328-55659703`) |
| `Strand` | Original strand of the reference sequence (`+` or `-`) |

> **Note:** All circRNAs are identified in this step; in general, those with coverage **above 30** are regarded as high-confidence.

## 2 Find modifications

### 2.1 Retrieve circRNA Raw Signals from FAST5

In this step, we use `fast5_subset.py` to extract FAST5 files containing circRNA reads identified in previous steps.

#### Usage example

```bash
python fast5_subset.py \
  -i /path/to/raw_fast5/ \
  -s /path/to/output_fast5/ \
  -l /path/to/read_id_list.txt \
  -r \
  -t 1
```
#### Parameters explaination in this workflow

- `-i` : Input directory containing the **original raw FAST5 files**.  
- `-s` : Output directory where **circRNA FAST5 files** will be saved.  
- `-l` : Text file containing **read IDs corresponding to circRNAs**.  
- `-r` : Flag to retain the original FAST5 folder structure.   
- `-t` : Number of threads to use (default: 1).  

### 2.2 Base calling

In this workflow, **Guppy** is used to perform basecalling on circRNA-related FAST5 files.

#### Usage example

```bash
guppy_basecaller \
  -i input_directory \
  -s output_directory \
  --num_callers 4 \
  --recursive \
  --fast5_out  \
  --flowcell FLO-MIN106 \
  --kit SQK-RNA002 \
  --device cuda:0
```

> **Note:** Guppy in this workflow is configured to use GPU (`--device cuda:0`) for faster basecalling. Make sure a compatible GPU is available; otherwise, Guppy will automatically use the CPU.

### 2.3 Convert multi-FAST5 to single-FAST5

Nanopore sequencing can produce multi-FAST5 files containing multiple reads per file. For downstream tools like Tombo or feature extraction scripts, it is often necessary to work with single-FAST5 files (one read per file). This step converts multi-FAST5 files into single-FAST5 format.

#### Usage Example

```bash
multi_to_single_fast5 \
  -i /path/to/multi_fast5/ \
  -s /path/to/single_fast5/ \
  --recursive
```

### 2.4 Signal re-alignment with Tombo (Resquiggle)

Tombo's `resquiggle` step aligns raw nanopore signals from FAST5 files to a reference genome. This is necessary for downstream modification detection and feature extraction.

#### Usage example

```bash
tombo resquiggle \
  /path/to/single_fast5/ \
  /path/to/reference_genome.fa \
  --overwrite \
  --basecall-group Basecall_1D_000 \
  --processes 5 \
  --fit-global-scale \
  --include-event-stdev \
  --ignore-read-locks
```

> **Note:** For example, in our human study, we use **hg38.fa** as the reference genome here, consistent with the circRNA coordinate analysis, ensuring that coordinates are consistent across all steps.

### 2.5 Signal feature extraction

```bash
python ./CircRM/Code/Feature_extraction.py \
  -i <input_fast5_dir> \
  -o <output_feature_dir> \
  --center_base <central_base> 
```
#### Parameters explaination

- `-i` : Input directory containing single-FAST5 files (output from Tombo resquiggle).  
- `-o` : Directory where extracted features will be saved.  
- `-t` : Number of CPU cores to use (default: 1).  
- `--group` : Tombo basecall group suffix (default: `RawGenomeCorrected_000`).  
- `--center_base` : Central base of the 5-mer filter (choose A, T, C, or G).  

These features combine genomic location (`chr`, `pos`, `strand`), read information (`read_id`, `idx`, `kmer`), and signal statistics (`mean`, `std`, `length`, `one-hot`).

### 2.6 Model training

We trained the model using IVET data with **XGBoost**, and hyperparameters were optimized via **RandomizedSearchCV**. You can directly use our pre-trained model located in the **Model** directory. If you do not need to train a model with your own data, you can **skip directly to 2.7**; otherwise, you can use the following code to train your own model.


```bash
python ./CircRM/Code/Train.py \
  -m <mod_csv_file> \
  -u <unmod_csv_file> \
  -o <output_model_dir> \
  -p <output_plot_dir> \
  -t <mod_type>
```
#### Dependencies

This step requires the following Python packages:  `xgboost`, `matplotlib`, `scikit-learn`.

#### Parameters explaination

- `-m` : Path to the CSV file containing features of positive (modified) samples.  
- `-u` : Path to the CSV file containing features of negative (unmodified) samples.  
- `-o` : Directory where the trained model will be saved.  
- `-p` : Directory where training plots will be saved.  
- `-t` : Modification type, e.g., `m5C`, `m6A`, `m1A`.  




### 2.7 Prediction

```bash
python ./CircRM/Code/Predict_multi-modif.py \
  -o <output_results_dir> \
  --models_dir <models_directory> \
  --tasks <mod_type>:<mode>:<threshold1,threshold2,...>:<input_feature_dir> \
  [--pvalue_config <config_file>]
```

#### Parameters explaination

- `-o` : Output directory for storing prediction results.  
- `--models_dir` : Directory containing all trained model files.  
- `--tasks` : One or more prediction tasks in the format:
   - `<mod_type>`: Modification type, e.g., `m5C`, `m6A`, `m1A`  
   - `<mode>`: Prediction mode (`pvalue` or `likelihood`)  
   - `<threshold1,threshold2,...>`: One or more decision thresholds, separated by commas, e.g., `0.001,0.01,0.05`.
   - `<input_feature_dir>`: Directory containing extracted features for prediction
- `--pvalue_config`*(optional)* : Path to a YAML or JSON file containing custom p-value threshold mappings. If not provided, the workflow will use the built-in thresholds corresponding to **p = 0.05, 0.01, and 0.001**.

#### Example

```bash
python ./CircRM/Code/Predict_multi-modif.py \
  -o /path/to/output_results_dir \
  --models_dir path/to/models_directory \
  --tasks \
      m5C:likelihood:5,10:/path/to/input_C_dir/ \
      m5C:pvalue:0.01:/path/to/input_C_dir/ \
      m6A:pvalue:0.05,0.001:/path/to/input_A_dir/ \
      ...
```
### 2.8 Methylation level

```bash
Rscript ./CircRM/Code/Methylation_level.R \
  -c <circRNA_file.txt> \
  -p <modification_pvalue.tsv> \
  -l <modification_likelihood.tsv> \
  -o <output_filtered_methylation.csv> \
  [--min_depth <min_read_depth>] \
  [--min_count <min_total_count>]
```

#### Dependencies

This script depends on the following R packages: `optparse`, `tidyr`, `dplyr`, `GenomicFeatures`, and `GenomicRanges`.

#### Parameters explaination

- `-c` : Path to the input circRNA file in TXT format.
- `-p` : Path to the modification p-value file in TSV format.
- `-l` : Path to the modification likelihood file in TSV format.
- `-o` : Path to the output CSV file for filtered methylation levels.
- `--min_depth` : Minimum read depth to keep a circRNA (default: 30).
- `--min_count` : Minimum total read count for a site (default: 5).  


